{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of mainNovi.ipynb","provenance":[{"file_id":"1RgpPqnfjDXnmAqv574fTKOnmwDleGJAh","timestamp":1591538782199},{"file_id":"13CoCcAxNj-gYS8dSWyRHzunsI2rS4BbG","timestamp":1591015809451},{"file_id":"1E5gGhZJIahb2iEiqyUtgO3XrqgkRFABM","timestamp":1589460082298}],"collapsed_sections":[],"authorship_tag":"ABX9TyP+zh557DIxojfY4m5/Ouq7"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jhys9xRZIuVn","colab_type":"text"},"source":["# **Connect google drive to the google colab**"]},{"cell_type":"code","metadata":{"id":"7xBiBaS-LYOX","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/google_drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tw97l1JMGtNK","colab_type":"code","colab":{}},"source":["file_path = \"google_drive/My Drive/Colab Notebooks\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Z9ja-hGI2Ik","colab_type":"text"},"source":["# **Install dependencies for rendering video in Google Colab**"]},{"cell_type":"code","metadata":{"id":"75_mS5rJeVw-","colab_type":"code","colab":{}},"source":["!pip install gym pyvirtualdisplay > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iF7eW9HOeYQy","colab_type":"code","colab":{}},"source":["!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ueSqoKZxh-Ye","colab_type":"code","colab":{}},"source":["!apt-get install x11-utils"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dkgWW9efhTn0","colab_type":"code","colab":{}},"source":["# RUN THIS IF YOU GET ERROR BASE NOT FOUND\n","!pip install pyglet --upgrade"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kt9o-Lboa3IM","colab_type":"code","colab":{}},"source":["import gym\n","from gym.wrappers import Monitor\n","import os\n","import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) #error only\n","\n","import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML,clear_output,Image\n","from IPython import display as ipythondisplay\n","\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TitEZd-Ea7dm","colab_type":"text"},"source":["# **Environment**"]},{"cell_type":"code","metadata":{"id":"-FH6ZPqWa-du","colab_type":"code","colab":{}},"source":["class Environment(object):\n","    def __init__(self,name, ACTIONS):\n","        self.env = gym.make(name)\n","        self.ACTIONS = ACTIONS\n","    def resetEnv(self,NUM_OF_FRAMES):\n","        self.env.reset()\n","        frames = []\n","        reward = 0\n","        done = False\n","        for i in range(NUM_OF_FRAMES):\n","            obs, r, d, info = self.env.step(self.ACTIONS[\"NOOP\"])\n","            reward += r\n","            done = done | d\n","            frame = preprocessFrame(obs, RESOLUTION)\n","            frames.append(frame)\n","        return frames, reward, done\n","    def step(self,action):\n","        observation, reward, done, info = self.env.step(action)\n","        return observation, reward, done, info\n","    def render(self):\n","        self.env.render()\n","        return\n","    def show_video(self):\n","        mp4list = glob.glob('video/*.mp4')\n","        mp4list = sorted(mp4list)\n","        if len(mp4list) > 0:\n","            mp4 = mp4list[-1]\n","            print(mp4)\n","            video = io.open(mp4, 'r+b').read()\n","            encoded = base64.b64encode(video)\n","            ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                        loop controls style=\"height: 400px;\">\n","                        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","                    </video>'''.format(encoded.decode('ascii'))))\n","        else: \n","            print(\"Could not find video\")\n","        return\n","    def wrap_env(self):\n","        self.env = Monitor(self.env, os.path.join(os.getcwd(),'video'), force=True)\n","        return "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cP2ywZV7bOBi","colab_type":"text"},"source":["# **Replay Buffer**"]},{"cell_type":"code","metadata":{"id":"0OWcfg3JbP9e","colab_type":"code","colab":{}},"source":["from numpy import random\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","class ReplayBuffer(object):\n","    def __init__(self,BUFFER_SIZE, RESOLUTION, NUM_OF_FRAMES):\n","        self.BUFFER_SIZE = BUFFER_SIZE\n","        self.RESOLUTION = RESOLUTION\n","        self.NUM_OF_FRAMES = NUM_OF_FRAMES\n","        self.buffer = []\n","    def add(self,s ,a ,r ,s_next, done):\n","        if (len(self.buffer) == self.BUFFER_SIZE): # if the buffer is full\n","            self.buffer.pop(0) # remove first element\n","        self.buffer.append((s,a,r,s_next,done))\n","    def size(self):\n","        return len(self.buffer)\n","    def sample(self, batch_size):\n","        if (len(self.buffer) >= batch_size):\n","            indicies = random.choice(self.size(), size=batch_size)\n","            samples=[]\n","            for ind in indicies:\n","                sample = self.buffer[ind]\n","                samples.append((unpackState(sample[0], self.RESOLUTION, self.NUM_OF_FRAMES),\n","                                sample[1],\n","                                sample[2],\n","                                unpackState(sample[3], self.RESOLUTION, self.NUM_OF_FRAMES),\n","                                sample[4]\n","                                ))\n","        else:\n","            raise ValueError(\"Replay buffer needs more data\")\n","        return samples"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sPvdlI7NbUgV","colab_type":"text"},"source":["# **Double Deep Q network**"]},{"cell_type":"code","metadata":{"id":"2nWSlsk4bY_D","colab_type":"code","colab":{}},"source":["from keras.layers import Input, Conv2D, Activation, Flatten, Dense\n","from keras.optimizers import Adam, RMSprop\n","from keras.models import load_model, Model\n","from keras import backend as K\n","import os \n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","class DQN(object):\n","    def __init__(self, RESOLUTION, NUM_OF_FRAMES, NUM_OF_ACTIONS, optimizer):\n","        self.network = self._createNetwork(RESOLUTION, NUM_OF_FRAMES, NUM_OF_ACTIONS, OPTIMIZER)\n","        self.target_network = self._createNetwork(RESOLUTION, NUM_OF_FRAMES, NUM_OF_ACTIONS, OPTIMIZER)\n","        self.target_network.set_weights(self.network.get_weights())\n","    def _createNetwork(self, RESOLUTION, NUM_OF_FRAMES, NUM_OF_ACTIONS, OPTIMIZER):\n","        inputs = Input(shape=(RESOLUTION, RESOLUTION, NUM_OF_FRAMES))\n","        X = Conv2D(filters=32, kernel_size=(8,8), strides=(4,4))(inputs)\n","        X = Activation('relu')(X)\n","        X = Conv2D(filters=64, kernel_size=(4,4), strides=(2,2))(X)\n","        X = Activation('relu')(X)\n","        X = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1))(X)\n","        X = Activation('relu')(X)\n","        X = Flatten()(X) \n","        X = Dense(units=512)(X)\n","        X = Activation('relu')(X)\n","        outputs = Dense(units=NUM_OF_ACTIONS)(X)\n","    \n","        model = Model(inputs=inputs, outputs=outputs)\n","        model.compile(optimizer=OPTIMIZER, loss=self.huber_loss)\n","        return model\n","    def huber_loss(self,a,b,in_keras=True):\n","        error = a - b\n","        quadratic_term = error*error / 2\n","        linear_term = abs(error) - 1/2\n","        use_linear_term = (abs(error) > 1.0)\n","        if in_keras:\n","            # Keras won't let us multiply floats by booleans, so we explicitly cast the booleans to floats\n","            use_linear_term = K.cast(use_linear_term, 'float32')\n","        return use_linear_term * linear_term + (1-use_linear_term) * quadratic_term\n","    def train_network(self, batch, BATCH_SIZE, GAMMA, NUM_OF_ACTIONS):\n","        s_batch, a_batch, r_batch, s_new_batch, d_batch = map(np.array, zip(*batch))\n","        target_batch = np.zeros((BATCH_SIZE, NUM_OF_ACTIONS))\n","        for j in range(BATCH_SIZE):\n","            target_batch[j,:] = self.network.predict(np.expand_dims(s_batch[j],axis=0), batch_size=1) # weights will be updated only for action a_batch[j]\n","            Q_target_next = self.target_network.predict(np.expand_dims(s_new_batch[j],axis=0), batch_size=1)\n","            if (d_batch[j] == True):\n","                target_batch[j, int(a_batch[j])] = r_batch[j]\n","            else: \n","                target_batch[j, int(a_batch[j])] = r_batch[j] + GAMMA*np.max(Q_target_next)\n","        loss = self.network.train_on_batch(s_batch, target_batch)\n","        return loss\n","    def update_target_network(self):\n","        self.target_network.set_weights(self.network.get_weights())\n","        return \n","    def save_networks(self,file_path):\n","        path = os.path.join(file_path,\"trained_model\")\n","        os.makedirs(path, exist_ok=True)\n","        save_path = os.path.join(path,\"model.h5\")\n","        self.network.save(save_path)\n","        print(\"The network is saved to {}\".format(save_path))\n","        save_path = os.path.join(path,\"target_model.h5\")\n","        self.target_network.save(save_path)\n","        print(\"Target network is saved to {}\".format(save_path))\n","    def load_networks(self,file_path):\n","        path = os.path.join(file_path,\"trained_model\")\n","        load_path = os.path.join(path,\"model.h5\")\n","        self.network = load_model(load_path,custom_objects={'huber_loss': self.huber_loss})\n","        print(\"The network is loaded from {}\".format(load_path))\n","        load_path = os.path.join(path,\"target_model.h5\")\n","        self.target_network = load_model(load_path,custom_objects={'huber_loss': self.huber_loss})\n","        print(\"Target network is loaded from {}\".format(load_path))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B4sF-U_yblbH","colab_type":"text"},"source":["# **Additional functions**"]},{"cell_type":"code","metadata":{"id":"-K9GU9-aczP0","colab_type":"code","colab":{}},"source":["import cv2\n","import numpy as np\n","from statistics import mean\n","import matplotlib.pyplot as plt\n","import pickle\n","import os\n","import sys\n","\n","def preprocessFrame(frame, RESOLUTION): \n","    # convert to gray and resize\n","    frame = cv2.cvtColor(cv2.resize(frame, (RESOLUTION, RESOLUTION)), cv2.COLOR_BGR2GRAY)\n","    # make it binary\n","    _, frame = cv2.threshold(frame,1,255,cv2.THRESH_BINARY)\n","    return frame\n","def packState(state):\n","    return np.packbits(state)\n","def unpackState(state, RESOLUTION, NUM_OF_FRAMES):\n","    return np.unpackbits(state).reshape(RESOLUTION,RESOLUTION,NUM_OF_FRAMES)\n","def stackFrames(frames):\n","    return np.stack(frames, axis=-1).astype(\"uint8\")\n","def average(data, frequency, start_avg_point):\n","    average_data = []\n","    if (len(data) > start_avg_point):\n","        round_num = -(len(str(frequency))-1)\n","        for i in range(0,round(len(data),round_num)-frequency,frequency):\n","            avg_data=mean(data[i:i+frequency])\n","            average_data.append(avg_data)\n","    return average_data\n","def save_to_disk(txt_path, values):\n","    if os.path.isfile(txt_path):\n","        os.remove(txt_path)\n","    with open(txt_path, \"wb\") as fp:   \n","        pickle.dump(values, fp)\n","    return\n","def load_from_disk(txt_path):\n","    with open(txt_path, \"rb\") as f:\n","        values =  pickle.load(f)\n","    return values\n","def plot_variable(values, xlabel, ylabel, title, save_path):\n","    fig = plt.figure(figsize=(12, 6))\n","    plt.xlabel(xlabel)\n","    plt.ylabel(ylabel)\n","    plt.title(title)\n","    plt.plot(values, figure=fig)\n","    plt.savefig(save_path)\n","    plt.show(block=False)\n","    return\n","def report(losses, episode_rewards, Q_values, file_path, replay_buffer, epsilon, eval_rewards, eval_Q_values):\n","    plots_dir = os.path.join(file_path,'plots')\n","    os.makedirs(plots_dir, exist_ok=True)\n","    # episode rewards\n","    save_path = os.path.join(plots_dir,\"reward.png\")\n","    plot_variable(episode_rewards, 'episodes', 'reward', 'Reward through episodes', save_path)\n","    # average reward\n","    average_rewards = average(episode_rewards, 100,200)\n","    if (len(average_rewards) != 0):\n","        save_path = os.path.join(plots_dir,\"rewardAveraged.png\")\n","        plot_variable(average_rewards, 'episodes', 'reward', 'Reward averaged over last 100 episodes', save_path)\n","    # average loss\n","    average_losses = average(losses, 1000, 20000)\n","    if (len(average_losses) != 0):\n","        save_path = os.path.join(plots_dir,\"lossAveraged.png\")\n","        plot_variable(average_losses, 'num of weight updates*1000', 'loss', 'Loss averaged over 1000 weight updates', save_path)       \n","    # average Q values\n","    average_Q = average(Q_values, 1000, 20000)\n","    if (len(average_Q) != 0):\n","        save_path = os.path.join(plots_dir,\"QvaluesAveraged.png\")\n","        plot_variable(average_Q, 'num of weight updates*1000', 'Q value', 'Q values averaged over 1000 weight updates', save_path)\n","    # eval rewards\n","    save_path = os.path.join(plots_dir,\"evalRewards.png\")\n","    plot_variable(eval_rewards, 'epochs', 'reward', 'Reward averaged over 30 episodes', save_path)\n","    save_path = os.path.join(plots_dir,\"evalQvalues.png\")\n","    plot_variable(eval_Q_values, 'epochs', 'Q value', 'Q values averaged over 30 episodes', save_path)\n","    # save variables to file\n","    txt_path = os.path.join(file_path,\"loss.txt\")\n","    save_to_disk(txt_path,losses)\n","    print(\"Loss is saved to {}\".format(txt_path))\n","    txt_path = os.path.join(file_path,\"Qvalues.txt\")\n","    save_to_disk(txt_path,Q_values)\n","    print(\"Q values are saved to {}\".format(txt_path))    \n","    txt_path = os.path.join(file_path,\"rewards.txt\")\n","    save_to_disk(txt_path,episode_rewards)\n","    print(\"Rewards are saved to {}\".format(txt_path))\n","    txt_path = os.path.join(file_path,\"replayBuffer.txt\")\n","    save_to_disk(txt_path,replay_buffer)\n","    print(\"Replay buffer is saved to {}\".format(txt_path))\n","    txt_path = os.path.join(file_path,\"epsilon.txt\")\n","    save_to_disk(txt_path,epsilon)\n","    print(\"Epsilon is saved to {}\".format(txt_path))\n","    txt_path = os.path.join(file_path,\"evalRewards.txt\")\n","    save_to_disk(txt_path,eval_rewards)\n","    print(\"Eval rewards are saved to {}\".format(txt_path))\n","    txt_path = os.path.join(file_path,\"evalQvalues.txt\")\n","    save_to_disk(txt_path,eval_Q_values)\n","    print(\"Eval Q values are saved to {}\".format(txt_path))\n","    return\n","def load_saved_data(file_path):\n","    # load saved variables\n","    txt_path = os.path.join(file_path,\"epsilon.txt\")\n","    epsilon = load_from_disk(txt_path)\n","    print(\"Epsilon is loaded from {}\".format(txt_path))\n","    txt_path = os.path.join(file_path,\"replayBuffer.txt\")\n","    replay_buffer = load_from_disk(txt_path)\n","    print(\"Replay Buffer is loaded from {}\".format(txt_path))\n","    txt_path = os.path.join(file_path,\"loss.txt\")\n","    losses = load_from_disk(txt_path)\n","    losses = list(losses)\n","    print(\"Loss is loaded from {}\".format(txt_path))\n","    txt_path = os.path.join(file_path,\"Qvalues.txt\")\n","    Q_values = load_from_disk(txt_path)\n","    print(\"Q values are loaded from {}\".format(txt_path))\n","    txt_path = os.path.join(file_path,\"rewards.txt\")\n","    episode_rewards = load_from_disk(txt_path)\n","    print(\"Rewards are loaded from {}\".format(txt_path))\n","    txt_path = os.path.join(file_path,\"evalRewards.txt\")\n","    eval_rewards = load_from_disk(txt_path)\n","    print(\"Eval rewards are loaded from {}\".format(txt_path))\n","    txt_path = os.path.join(file_path,\"evalQvalues.txt\")\n","    eval_Q_values = load_from_disk(txt_path)\n","    print(\"Eval Q values are loaded from {}\".format(txt_path))\n","    num_of_weight_updates = len(losses)\n","    return epsilon, replay_buffer, losses, Q_values, episode_rewards, num_of_weight_updates, eval_rewards, eval_Q_values"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T41oSvWaTIpO","colab_type":"text"},"source":["# **Hyperparameters**\n"]},{"cell_type":"code","metadata":{"id":"kM6bG076Pj7o","colab_type":"code","colab":{}},"source":["ACTIONS = {\"NOOP\":0,\"FIRE\":1,\"RIGHT\":2,\"LEFT\":3,\"RIGHTFIRE\":4,\"LEFTFIRE\":5}\n","NUM_OF_FRAMES = 4 # number of frames that make 1 state\n","EPISODES = 10000 # number of episodes\n","BUFFER_SIZE = 300000 # size of the replay buffer(can not put bigger size, RAM)\n","MIN_OBSERVATIONS = 30000\n","RESOLUTION = 84 # resolution of frames\n","BATCH_SIZE = 32\n","EPSILON_START = 1 # starting value for the exploration probability\n","EPSILON_END = 0.1 \n","FINAL_EXPLORATION_STATE = 500000 # final frame for which epsilon is decayed\n","GAMMA = 0.99 # discount factor\n","TARGET_NETWORK_UPDATE_FREQ = 40000 \n","REPORT_EPISODE_FREQ = 100\n","TRAINING_FREQUENCY = 4\n","OPTIMIZER = RMSprop(lr=0.00025,rho=0.95,epsilon=0.01)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UET8Y4YgiUh_","colab_type":"text"},"source":["# **Agent**"]},{"cell_type":"code","metadata":{"id":"J5Og3RVvifj0","colab_type":"code","colab":{}},"source":["from statistics import mean\n","\n","class Agent(object):\n","\n","    def EGreedyPolicy(self,Q, state, epsilon, NUM_OF_ACTIONS):\n","        Q_values = Q.network.predict(np.expand_dims(state,axis=0), batch_size=1)\n","        if (np.random.random() < epsilon): # exploration\n","            action = np.random.randint(0, NUM_OF_ACTIONS)\n","            is_greedy_action = False\n","        else:  # exploitation\n","            action = np.argmax(Q_values)\n","            is_greedy_action = True\n","        return action, Q_values[0,action], is_greedy_action\n","    def decayEpsilon(self, epsilon, start, end, states_num):\n","        d_epsilon = (start-end)/states_num\n","        epsilon = epsilon - d_epsilon\n","        return epsilon\n","    def getCustomReward(self, reward, info, LIFE_CHECKPOINT):\n","        if reward > 0:\n","            reward = 1\n","        if 'ale.lives' in info is not None:\n","            if info['ale.lives'] != LIFE_CHECKPOINT: # if you lost a life\n","                if LIFE_CHECKPOINT == 0: # when starting a new episode\n","                    LIFE_CHECKPOINT = 3\n","                    reward = 0\n","                else: # get punishment\n","                    LIFE_CHECKPOINT = info['ale.lives']\n","                    reward = -1\n","        else:\n","            print(\"Lives is None\")\n","            if LIFE_CHECKPOINT != 0:\n","                LIFE_CHECKPOINT = 0\n","                reward = -1\n","            else:\n","                reward = 0\n","        return reward, LIFE_CHECKPOINT\n","    def train(self, Q, space_invaders, file_path, continue_training, EPISODE=0):\n","        if continue_training:\n","            epsilon, replay_buffer, losses, Q_values, episode_rewards, num_of_weight_updates, eval_rewards, eval_Q_values = load_saved_data(file_path)\n","        else:\n","            epsilon = EPSILON_START\n","            num_of_weight_updates = 0\n","            episode_rewards = []\n","            losses = []\n","            Q_values = [] \n","            eval_rewards = []\n","            eval_Q_values = []\n","            replay_buffer = ReplayBuffer(BUFFER_SIZE, RESOLUTION, NUM_OF_FRAMES)\n","        \n","        frame_count = 0\n","        LIFE_CHECKPOINT = 3\n","        for episode in range(EPISODE,EPISODES):\n","            # reset the environment and init variables\n","            frames, _, _ = space_invaders.resetEnv(NUM_OF_FRAMES)\n","            state = stackFrames(frames)\n","            done = False\n","            episode_reward = 0\n","            episode_reward_clipped = 0\n","            frames_buffer = frames # contains preprocessed frames (not stacked)\n","            while not done:\n","                if (episode % REPORT_EPISODE_FREQ == 0):\n","                    space_invaders.render()\n","                # select an action from behaviour policy\n","                action, Q_value, is_greedy_action = self.EGreedyPolicy(Q, state, epsilon, len(ACTIONS))\n","                # perform action in the environment\n","                observation, reward, done, info = space_invaders.step(action)\n","                episode_reward += reward # update episode reward\n","                reward, LIFE_CHECKPOINT = self.getCustomReward(reward, info, LIFE_CHECKPOINT)\n","                episode_reward_clipped += reward\n","                frame = preprocessFrame(observation, RESOLUTION)\n","                # pop first frame from the buffer, and add new at the end (s1=[f1,f2,f3,f4], s2=[f2,f3,f4,f5])\n","                frames_buffer.append(frame) \n","                frames_buffer.pop(0)\n","                new_state = stackFrames(frames_buffer)\n","                # add (s,a,r,s') tuple to the replay buffer\n","                replay_buffer.add(packState(state), action, reward, packState(new_state), done)\n","                \n","                state = new_state # new state becomes current state\n","                frame_count += 1\n","                if (replay_buffer.size() > MIN_OBSERVATIONS): # if there is enough data in replay buffer\n","                    Q_values.append(Q_value)\n","                    if (frame_count % TRAINING_FREQUENCY == 0):\n","                        batch = replay_buffer.sample(BATCH_SIZE)\n","                        loss = Q.train_network(batch, BATCH_SIZE, GAMMA, len(ACTIONS))\n","                        losses.append(loss)\n","                        num_of_weight_updates += 1\n","                    if (epsilon > EPSILON_END):\n","                        epsilon = self.decayEpsilon(epsilon, EPSILON_START, EPSILON_END, FINAL_EXPLORATION_STATE)\n","                if (num_of_weight_updates % TARGET_NETWORK_UPDATE_FREQ == 0) and (num_of_weight_updates != 0): # update weights of target network\n","                    Q.update_target_network() \n","                    print(\"Target_network is updated!\")\n","            episode_rewards.append(episode_reward)\n","\n","            print(\"EPISODE {}\".format(episode))\n","            print(\"reward: {}\".format(episode_reward))\n","            print(\"Clipped reward: {}\".format(episode_reward_clipped))\n","            print(\"epsilon: {}\".format(epsilon))\n","            print(\"buffer size: {}\".format(replay_buffer.size()))\n","            if (episode % REPORT_EPISODE_FREQ == 0):\n","                space_invaders.show_video()\n","                _, _, eval_reward, eval_Q_value = self.play_game(0.05, Q, space_invaders, 30)\n","                eval_rewards.append(eval_reward)\n","                eval_Q_values.append(eval_Q_value)\n","                Q.save_networks(file_path)\n","                report(losses, episode_rewards, Q_values, file_path, replay_buffer, epsilon, eval_rewards, eval_Q_values)\n","    def play_game(self,epsilon, Q, space_invaders, episodes, load_pretrained=False, render=False):\n","        print(\"Playing the game!\")\n","        if load_pretrained:\n","            Q.load_networks(file_path)\n","        episode_rewards = []\n","        Q_values = []\n","        for episode in range(episodes):\n","            frames, reward, _ = space_invaders.resetEnv(NUM_OF_FRAMES)\n","            state = stackFrames(frames)\n","            done = False\n","            episode_reward = 0\n","            frames_buffer = frames\n","            while not done:\n","                # select an action from behaviour policy\n","                action, Q_value, is_greedy_action = self.EGreedyPolicy(Q, state, epsilon, len(ACTIONS))\n","                if render:\n","                    space_invaders.render()\n","                # perform action in the environment\n","                observation, reward, done, info = space_invaders.step(action)\n","                # pop first frame from the buffer, and add new at the end (s1=[f1,f2,f3,f4], s2=[f2,f3,f4,f5])\n","                frame = preprocessFrame(observation, RESOLUTION)\n","                frames_buffer.append(frame) \n","                frames_buffer.pop(0)\n","                new_state = stackFrames(frames_buffer)\n","                state = new_state # new state becomes current state\n","\n","                episode_reward += reward # update episode reward\n","                Q_values.append(Q_value)\n","            print(\"EPSIODE {}\".format(episode))\n","            print(\"episode reward: {}\".format(episode_reward))\n","            episode_rewards.append(episode_reward)\n","        return episode_rewards, Q_values, mean(episode_rewards), mean(Q_values)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G04txDF0TQSt","colab_type":"text"},"source":["# **Start training**"]},{"cell_type":"code","metadata":{"id":"QseYLwW-TTHg","colab_type":"code","colab":{}},"source":["import gym\n","import numpy as np\n","import sys\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","space_invaders = Environment('SpaceInvaders-v0',ACTIONS)\n","space_invaders.wrap_env()\n","Q = DQN(RESOLUTION, NUM_OF_FRAMES, len(ACTIONS), OPTIMIZER)\n","\n","agent = Agent()\n","agent.train(Q, space_invaders, file_path, continue_training=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Equo-2StL9bE","colab_type":"text"},"source":["# **Continue training**"]},{"cell_type":"code","metadata":{"id":"9TWSn0f0MVBY","colab_type":"code","colab":{}},"source":["import gym\n","import numpy as np\n","import sys\n","\n","space_invaders = Environment('SpaceInvaders-v0',ACTIONS)\n","space_invaders.wrap_env()\n","Q = DQN(RESOLUTION, NUM_OF_FRAMES, len(ACTIONS), OPTIMIZER)\n","Q.load_networks(file_path)\n","\n","agent = Agent()\n","agent.train(Q, space_invaders, file_path, continue_training=True, EPISODE=2400)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IiAj_bDJMXkj","colab_type":"text"},"source":["# **Play the game**"]},{"cell_type":"code","metadata":{"id":"JbkDtrmIMZfg","colab_type":"code","colab":{}},"source":["epsilon = 0\n","episodes = 3\n","space_invaders = Environment('SpaceInvaders-v0',ACTIONS)\n","space_invaders.wrap_env()\n","Q = DQN(RESOLUTION, NUM_OF_FRAMES, len(ACTIONS), OPTIMIZER)\n","\n","agent = Agent()\n","e_rewards, q_values, mean_reward, mean_q_value = agent.play_game(epsilon, Q, space_invaders, episodes, load_pretrained=True, render=True)\n","space_invaders.show_video()\n","print(\"Average episode reward is {}\".format(mean_reward))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nVxxdBt1x3rN","colab_type":"text"},"source":["# DEBUG\n"]},{"cell_type":"code","metadata":{"id":"pCiSdfx-NKfK","colab_type":"code","colab":{}},"source":["# CHECK the packState and unpackState (what goes in and out of replay buffer)\n","\n","space_invaders = Environment('SpaceInvaders-v0',ACTIONS)\n","space_invaders.wrap_env()\n","d = True\n","for i in range(1000):\n","    if d:\n","       frames,_,_ = space_invaders.resetEnv(NUM_OF_FRAMES)\n","       frames_buffer=frames\n","    o, r,d, info = space_invaders.step(np.random.randint(0,6))\n","    if i == 500:\n","        state = stackFrames(frames_buffer)\n","    frame = preprocessFrame(o,RESOLUTION)\n","    frames_buffer.append(frame)\n","    frames_buffer.pop(0)\n","packed_state = packState(state)\n","unpacked_state = unpackState(packed_state,RESOLUTION, NUM_OF_FRAMES)\n","for i in range(NUM_OF_FRAMES):\n","    plt.figure()\n","    plt.imshow(state[:,:,i])\n","    plt.figure()\n","    plt.imshow(unpacked_state[:,:,i])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c-O83DevvYeE","colab_type":"code","colab":{}},"source":["# Check target_network and network \n"],"execution_count":0,"outputs":[]}]}